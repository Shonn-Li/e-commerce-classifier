{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430b90ea",
   "metadata": {},
   "source": [
    "## E-commerce classifier\n",
    "### Instruction\n",
    "I have ran this on my M1 macbook pro 13.3 inch with GPU optimization in jupyter notebook, the data like train.csv should be in the same directory as the kaggle ipynb file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b777f",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d2ed85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm.keras import TqdmCallback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696c3ac",
   "metadata": {},
   "source": [
    "### Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dcda15cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preprocessing categorical features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shonnli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/shonnli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/shonnli/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text data...\n",
      "Preprocessing text data...\n",
      "Preprocessing image data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 21627/21627 [00:06<00:00, 3525.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing image data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 21628/21628 [00:06<00:00, 3443.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DAE for image features...\n",
      "Epoch 1/10\n",
      "169/169 [==============================] - 40s 185ms/step - loss: 0.0232\n",
      "Epoch 2/10\n",
      "169/169 [==============================] - 36s 213ms/step - loss: 0.0146\n",
      "Epoch 3/10\n",
      "169/169 [==============================] - 33s 197ms/step - loss: 0.0126\n",
      "Epoch 4/10\n",
      "169/169 [==============================] - 29s 171ms/step - loss: 0.0115\n",
      "Epoch 5/10\n",
      "169/169 [==============================] - 41s 241ms/step - loss: 0.0108\n",
      "Epoch 6/10\n",
      "169/169 [==============================] - 39s 233ms/step - loss: 0.0103\n",
      "Epoch 7/10\n",
      "169/169 [==============================] - 38s 227ms/step - loss: 0.0098\n",
      "Epoch 8/10\n",
      "169/169 [==============================] - 36s 215ms/step - loss: 0.0095\n",
      "Epoch 9/10\n",
      "169/169 [==============================] - 45s 265ms/step - loss: 0.0093\n",
      "Epoch 10/10\n",
      "169/169 [==============================] - 32s 190ms/step - loss: 0.0090\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Check for GPU availability\n",
    "import tensorflow as tf\n",
    "\n",
    "cuda = tf.config.list_physical_devices('GPU')\n",
    "device = '/gpu:0' if cuda else '/cpu:0'\n",
    "\n",
    "# preprocess categorical features\n",
    "def preprocess_categorical_features(df):\n",
    "    categorical_columns = ['gender', 'baseColour', 'season', 'usage']\n",
    "    for column in categorical_columns:\n",
    "        label_encoder = LabelEncoder()\n",
    "        df[column] = label_encoder.fit_transform(df[column].astype(str))\n",
    "    return df\n",
    "\n",
    "print(\"Preprocessing categorical features...\")\n",
    "train_df = preprocess_categorical_features(train_df)\n",
    "test_df = preprocess_categorical_features(test_df)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text_data(df):\n",
    "    print(\"Preprocessing text data...\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Convert to lowercase, remove special characters and numbers, tokenize, remove stopwords, and lemmatize\n",
    "    df['processedText'] = df['noisyTextDescription'].str.lower()\n",
    "    df['processedText'] = df['processedText'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "    df['processedText'] = df['processedText'].apply(lambda x: ' '.join([lemmatizer.lemmatize(w) for w in word_tokenize(x) if w not in stop_words]))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    text_features = vectorizer.fit_transform(df['processedText'])\n",
    "    text_features_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    return text_features_df\n",
    "\n",
    "train_text_features = preprocess_text_data(train_df)\n",
    "test_text_features = preprocess_text_data(test_df)\n",
    "\n",
    "\n",
    "# preprocess image data\n",
    "def preprocess_image_data(df, image_folder_path):\n",
    "    print(\"Preprocessing image data...\")\n",
    "    image_data = []\n",
    "    for image_id in tqdm(df['id'], total=df.shape[0]):\n",
    "        image_path = os.path.join(image_folder_path, f\"{image_id}.jpg\")\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, (60, 80)) / 255.0\n",
    "        image_data.append(image.flatten())\n",
    "    image_data_np = np.array(image_data)\n",
    "    image_data_df = pd.DataFrame(image_data_np)\n",
    "    return image_data_df\n",
    "\n",
    "train_image_features = preprocess_image_data(train_df, 'noisy-images/noisy-images')\n",
    "test_image_features = preprocess_image_data(test_df, 'noisy-images/noisy-images')\n",
    "\n",
    "\n",
    "# DAE for image features\n",
    "def create_dae(input_dim):\n",
    "    dae_input = Input(shape=(input_dim,))\n",
    "    x = Dense(1024, activation='relu')(dae_input)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    dae_output = Dense(input_dim, activation='sigmoid')(x)\n",
    "    dae = Model(dae_input, dae_output)\n",
    "    dae.compile(optimizer='adam', loss='mse')\n",
    "    return dae\n",
    "\n",
    "# Train DAE for image features\n",
    "print(\"Training DAE for image features...\")\n",
    "image_dae = create_dae(train_image_features.shape[1])\n",
    "with tf.device(device):\n",
    "    image_dae.fit(train_image_features, train_image_features, epochs=10, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc39a3ef",
   "metadata": {},
   "source": [
    "### data spliting for training, validation, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5ed45878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the data into train and validation sets...\n",
      "Processing target labels for train and validation sets...\n",
      "Preparing data for CNN and LSTM...\n",
      "X_train_img_array shape: (19464, 60, 80, 3)\n",
      "y_train_categorical shape: (19464, 27)\n",
      "X_val_img_array shape: (2163, 60, 80, 3)\n",
      "y_val_categorical shape: (2163, 27)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and validation sets\n",
    "print(\"Splitting the data into train and validation sets...\")\n",
    "train_df, val_df, train_image_features, val_image_features, train_text_features, val_text_features = train_test_split(\n",
    "    train_df, \n",
    "    train_image_features, \n",
    "    train_text_features,\n",
    "    test_size=0.1, \n",
    "    random_state=42, \n",
    "    stratify=train_df[\"category\"]\n",
    ")\n",
    "\n",
    "# Get the target labels for train and validation sets\n",
    "print(\"Processing target labels for train and validation sets...\")\n",
    "y_train = train_df[\"category\"]\n",
    "\n",
    "X_val = val_df.drop(columns=[\"id\", \"category\", \"noisyTextDescription\"])\n",
    "y_val = val_df[\"category\"]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_val = label_encoder.transform(y_val)\n",
    "\n",
    "# One-hot encode target labels\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_val_categorical = to_categorical(y_val)\n",
    "\n",
    "# Preprocess for CNN, Bert\n",
    "print(\"Preparing data for CNN and LSTM...\")\n",
    "X_train_img_array = np.array(train_image_features).reshape(-1, 60, 80, 3)\n",
    "X_val_img_array = np.array(val_image_features).reshape(-1, 60, 80, 3)\n",
    "\n",
    "# Prepare data for Bert\n",
    "X_train_text = train_text_features\n",
    "X_val_text = val_text_features\n",
    "\n",
    "print(\"X_train_img_array shape:\", X_train_img_array.shape)\n",
    "print(\"y_train_categorical shape:\", y_train_categorical.shape)\n",
    "print(\"X_val_img_array shape:\", X_val_img_array.shape)\n",
    "print(\"y_val_categorical shape:\", y_val_categorical.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48e00cb",
   "metadata": {},
   "source": [
    "### Train Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c56dc5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_img_array shape: (19464, 60, 80, 3)\n",
      "y_train_categorical shape: (19464, 27)\n",
      "X_val_img_array shape: (2163, 60, 80, 3)\n",
      "y_val_categorical shape: (2163, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "153/153 [==============================] - 28s 147ms/step - loss: 1.5520 - accuracy: 0.6261 - val_loss: 4.9796 - val_accuracy: 0.0231 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "153/153 [==============================] - 16s 104ms/step - loss: 1.1768 - accuracy: 0.7207 - val_loss: 2.6793 - val_accuracy: 0.1479 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "153/153 [==============================] - 17s 110ms/step - loss: 1.0624 - accuracy: 0.7484 - val_loss: 1.8960 - val_accuracy: 0.4933 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "153/153 [==============================] - 18s 114ms/step - loss: 0.9588 - accuracy: 0.7736 - val_loss: 1.4623 - val_accuracy: 0.6422 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "153/153 [==============================] - 17s 109ms/step - loss: 0.8791 - accuracy: 0.7867 - val_loss: 1.3147 - val_accuracy: 0.6801 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "153/153 [==============================] - 18s 119ms/step - loss: 0.7904 - accuracy: 0.8081 - val_loss: 1.5138 - val_accuracy: 0.6260 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "153/153 [==============================] - 16s 104ms/step - loss: 0.7058 - accuracy: 0.8206 - val_loss: 1.2305 - val_accuracy: 0.7346 - lr: 0.0010\n",
      "Epoch 8/15\n",
      "153/153 [==============================] - 21s 139ms/step - loss: 0.5827 - accuracy: 0.8429 - val_loss: 1.4152 - val_accuracy: 0.6731 - lr: 0.0010\n",
      "Epoch 9/15\n",
      "153/153 [==============================] - 17s 111ms/step - loss: 0.4601 - accuracy: 0.8684 - val_loss: 1.7228 - val_accuracy: 0.6445 - lr: 0.0010\n",
      "Epoch 10/15\n",
      "153/153 [==============================] - 19s 122ms/step - loss: 0.3560 - accuracy: 0.8967 - val_loss: 1.6351 - val_accuracy: 0.6417 - lr: 0.0010\n",
      "Epoch 11/15\n",
      "153/153 [==============================] - 16s 104ms/step - loss: 0.1769 - accuracy: 0.9470 - val_loss: 1.4873 - val_accuracy: 0.7712 - lr: 1.0000e-04\n",
      "Epoch 12/15\n",
      "153/153 [==============================] - 16s 107ms/step - loss: 0.0950 - accuracy: 0.9736 - val_loss: 1.4646 - val_accuracy: 0.7707 - lr: 1.0000e-04\n",
      "Epoch 13/15\n",
      "153/153 [==============================] - 17s 109ms/step - loss: 0.0674 - accuracy: 0.9826 - val_loss: 1.5897 - val_accuracy: 0.7693 - lr: 1.0000e-04\n",
      "Epoch 14/15\n",
      "153/153 [==============================] - 17s 113ms/step - loss: 0.0499 - accuracy: 0.9893 - val_loss: 1.6966 - val_accuracy: 0.7614 - lr: 1.0000e-04\n",
      "Epoch 15/15\n",
      "153/153 [==============================] - 20s 131ms/step - loss: 0.0382 - accuracy: 0.9924 - val_loss: 1.7281 - val_accuracy: 0.7628 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "class CNN:\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(InputLayer(input_shape=input_shape))\n",
    "        self.model.add(Reshape((60, 80, 3), input_shape=input_shape))\n",
    "        self.model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\"))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\"))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(Conv2D(128, kernel_size=(3, 3), activation=\"relu\"))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(Conv2D(256, kernel_size=(3, 3), activation=\"relu\"))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(512, activation=\"relu\"))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "        self.model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs, batch_size):\n",
    "        def lr_schedule(epoch, lr):\n",
    "            if epoch % 10 == 0 and epoch > 0:\n",
    "                lr = lr * 0.1\n",
    "            return lr\n",
    "        \n",
    "        lr_callback = LearningRateScheduler(lr_schedule)\n",
    "        self.model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, callbacks=[lr_callback])\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test)\n",
    "\n",
    "# Train CNN for images\n",
    "print(\"X_train_img_array shape:\", X_train_img_array.shape)\n",
    "print(\"y_train_categorical shape:\", y_train_categorical.shape)\n",
    "print(\"X_val_img_array shape:\", X_val_img_array.shape)\n",
    "print(\"y_val_categorical shape:\", y_val_categorical.shape)\n",
    "# Train CNN for images\n",
    "cnn_model = CNN((60, 80, 3), len(np.unique(y_train)))\n",
    "with tf.device(device):\n",
    "    cnn_model.train(X_train_img_array, y_train_categorical, X_val_img_array, y_val_categorical, epochs=15, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7fb21e",
   "metadata": {},
   "source": [
    "### Train Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c6448b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERT-based model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1217/1217 [==============================] - 776s 621ms/step - loss: 1.5125 - accuracy: 0.6441 - val_loss: 1.0723 - val_accuracy: 0.7610\n",
      "Epoch 2/3\n",
      "1217/1217 [==============================] - 725s 595ms/step - loss: 1.0651 - accuracy: 0.7713 - val_loss: 0.9879 - val_accuracy: 0.7878\n",
      "Epoch 3/3\n",
      "1217/1217 [==============================] - 755s 621ms/step - loss: 0.9397 - accuracy: 0.7965 - val_loss: 0.9584 - val_accuracy: 0.7933\n",
      "\n",
      "Training history:\n",
      "loss: [1.5125396251678467, 1.065078854560852, 0.9396510124206543]\n",
      "accuracy: [0.644112229347229, 0.7712700366973877, 0.7964960932731628]\n",
      "val_loss: [1.0723285675048828, 0.9879311323165894, 0.9584036469459534]\n",
      "val_accuracy: [0.7609801292419434, 0.7877947092056274, 0.7933425903320312]\n",
      "\n",
      "BERT model training finished\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TFBertModel, BertTokenizer, BertConfig\n",
    "\n",
    "from transformers import TFDistilBertModel, DistilBertTokenizer, DistilBertConfig\n",
    "\n",
    "# Load BERT tokenizer and BERT model\n",
    "bert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert_config = DistilBertConfig.from_pretrained('distilbert-base-uncased', output_hidden_states=True)\n",
    "bert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=bert_config)\n",
    "\n",
    "# Preprocess text data with BERT tokenizer\n",
    "def bert_encode(texts, tokenizer, max_len=64):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        tokens = tokenizer.encode_plus(text, max_length=max_len, truncation=True,\n",
    "                                       padding='max_length', add_special_tokens=True,\n",
    "                                       return_attention_mask=True, return_tensors='tf')\n",
    "        input_ids.append(tokens['input_ids'][0])  # Change this line\n",
    "        attention_masks.append(tokens['attention_mask'][0])  # Change this line\n",
    "\n",
    "    return np.array(input_ids), np.array(attention_masks)\n",
    "\n",
    "# Define BERT-based model\n",
    "def build_bert_model(bert_model, num_classes):\n",
    "    input_ids = Input(shape=(64,), dtype=tf.int32, name='input_ids')\n",
    "    attention_masks = Input(shape=(64,), dtype=tf.int32, name='attention_masks')\n",
    "\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_masks)[0]  # Change the index to 0\n",
    "    x = Dropout(0.2)(bert_output[:, 0, :])\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[input_ids, attention_masks], outputs=output)  # Update the variable name here\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Preprocess text data\n",
    "train_texts = train_df['noisyTextDescription'].values\n",
    "val_texts = val_df['noisyTextDescription'].values\n",
    "X_train_text_ids, X_train_text_masks = bert_encode(train_texts, bert_tokenizer)\n",
    "X_val_text_ids, X_val_text_masks = bert_encode(val_texts, bert_tokenizer)\n",
    "\n",
    "# Train BERT-based model\n",
    "print(\"Training BERT-based model...\")\n",
    "bert_based_model = build_bert_model(bert_model, len(np.unique(y_train)))\n",
    "history = bert_based_model.fit([X_train_text_ids, X_train_text_masks], y_train_categorical,\n",
    "                               validation_data=([X_val_text_ids, X_val_text_masks], y_val_categorical),\n",
    "                               epochs=3, batch_size=16,\n",
    "                               callbacks=[tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)])\n",
    "\n",
    "# Print training progress\n",
    "print(\"\\nTraining history:\")\n",
    "for key, values in history.history.items():\n",
    "    print(f\"{key}: {values}\")\n",
    "\n",
    "print('\\nBERT model training finished')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1023d6b",
   "metadata": {},
   "source": [
    "### Train Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a15f38a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_gb shape: (19464, 4)\n",
      "y_train shape: (19464,)\n",
      "X_val_gb shape: (2163, 4)\n",
      "X_test_gb shape: (21628, 4)\n",
      "start GB fitting\n",
      "finished GB fitting\n",
      "Validation accuracy: 0.5376791493296348\n",
      "Validation loss: 1.4741624269915423\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = ['gender', 'baseColour', 'season', 'usage']\n",
    "X_train_gb = train_df[categorical_columns]\n",
    "X_val_gb = val_df[categorical_columns]\n",
    "X_test_gb = test_df[categorical_columns]\n",
    "\n",
    "print(\"X_train_gb shape:\", X_train_gb.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val_gb shape:\", X_val_gb.shape)\n",
    "print(\"X_test_gb shape:\", X_test_gb.shape)\n",
    "\n",
    "# Train Gradient Boosting for categorical features\n",
    "gb_model = GradientBoostingClassifier()\n",
    "print(\"start GB fitting\")\n",
    "gb_model.fit(X_train_gb, y_train)\n",
    "print(\"finished GB fitting\")\n",
    "\n",
    "accuracy = gb_model.score(X_val_gb, y_val)\n",
    "print(\"Validation accuracy:\", accuracy)\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "y_pred = gb_model.predict_proba(X_val_gb)\n",
    "loss = log_loss(y_val, y_pred)\n",
    "print(\"Validation loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f1a15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "categorical_columns = ['gender', 'baseColour', 'season', 'usage']\n",
    "X_train_gb = train_df[categorical_columns]\n",
    "X_val_gb = val_df[categorical_columns]\n",
    "X_test_gb = test_df[categorical_columns]\n",
    "\n",
    "print(\"X_train_gb shape:\", X_train_gb.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val_gb shape:\", X_val_gb.shape)\n",
    "print(\"X_test_gb shape:\", X_test_gb.shape)\n",
    "\n",
    "# Hyperparameters to tune\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'min_samples_split': [2, 3, 4, 5],\n",
    "    'min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "gb_model = GradientBoostingClassifier()\n",
    "\n",
    "# Randomized search\n",
    "random_search = RandomizedSearchCV(gb_model, param_distributions=param_dist, n_iter=20, cv=3, n_jobs=-1, verbose=1)\n",
    "print(\"start GB fitting\")\n",
    "random_search.fit(X_train_gb, y_train)\n",
    "print(\"finished GB fitting\")\n",
    "\n",
    "# Best model\n",
    "best_gb_model = random_search.best_estimator_\n",
    "\n",
    "# Training the best model with a progress bar\n",
    "n_estimators = best_gb_model.n_estimators\n",
    "best_gb_model.n_iter_no_change = 10\n",
    "best_gb_model.warm_start = True\n",
    "\n",
    "# Shuffle the dataset\n",
    "X_train_gb_shuffled, y_train_shuffled = shuffle(X_train_gb, y_train, random_state=42)\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress_bar = tqdm(range(n_estimators), desc=\"Training progress\")\n",
    "\n",
    "# Train the model incrementally and update the progress bar\n",
    "for i in progress_bar:\n",
    "    best_gb_model.fit(X_train_gb_shuffled, y_train_shuffled)\n",
    "    progress_bar.set_postfix_str(f\"Training accuracy: {best_gb_model.train_score_[-1]:.4f}\")\n",
    "    if best_gb_model.n_estimators_ < i + 1:\n",
    "        break\n",
    "progress_bar.close()\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = best_gb_model.score(X_val_gb, y_val)\n",
    "print(\"Validation accuracy:\", accuracy)\n",
    "\n",
    "y_pred = best_gb_model.predict_proba(X_val_gb)\n",
    "loss = log_loss(y_val, y_pred)\n",
    "print(\"Validation loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e48c5b",
   "metadata": {},
   "source": [
    "### Data preprocessing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1febf521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with CNN...\n",
      "68/68 [==============================] - 2s 19ms/step\n",
      "Predicting with BERT...\n",
      "39/68 [================>.............] - ETA: 6s"
     ]
    }
   ],
   "source": [
    "test_texts = test_df['noisyTextDescription'].values\n",
    "X_test_text_ids, X_test_text_masks = bert_encode(test_texts, bert_tokenizer)\n",
    "# Ensemble predictions\n",
    "print(\"Predicting with CNN...\")\n",
    "cnn_pred = cnn_model.model.predict(X_val_img_array)\n",
    "print(\"Predicting with BERT...\")\n",
    "bert_pred = bert_based_model.predict([X_val_text_ids, X_val_text_masks])\n",
    "print(\"Predicting with Gradient Boosting...\")\n",
    "gb_pred = gb_model.predict_proba(X_val_gb)\n",
    "weights = [0.382, 0.382, 0.236]\n",
    "print(\"Ensembling predictions with weighted averaging...\")\n",
    "ensemble_pred = weights[0] * cnn_pred + weights[1] * bert_pred + weights[2] * gb_pred\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "ensemble_pred_labels = np.argmax(ensemble_pred, axis=1)\n",
    "\n",
    "# Calculate ensemble accuracy\n",
    "ensemble_accuracy = accuracy_score(y_val, ensemble_pred_labels)\n",
    "print(f\"Ensemble accuracy: {ensemble_accuracy}\")\n",
    "\n",
    "# Make predictions on test set\n",
    "print(\"Predicting test set with CNN...\")\n",
    "cnn_test_pred = cnn_model.model.predict(test_image_features.to_numpy().reshape(-1, 60, 80, 3))\n",
    "print(\"Predicting with BERT...\")\n",
    "bert_test_pred = bert_based_model.predict([X_test_text_ids, X_test_text_masks])\n",
    "print(\"Predicting test set with Gradient Boosting...\")\n",
    "gb_test_pred = gb_model.predict_proba(X_test_gb)\n",
    "\n",
    "print(\"Ensembling test set predictions...\")\n",
    "ensemble_test_pred = weights[0] * cnn_test_pred + weights[1] * bert_test_pred + weights[2] * gb_test_pred\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "ensemble_test_labels = np.argmax(ensemble_test_pred, axis=1)\n",
    "\n",
    "# Save predictions to CSV\n",
    "print(\"Saving predictions to result.csv...\")\n",
    "test_df['category'] = label_encoder.inverse_transform(ensemble_test_labels)\n",
    "test_df[['id', 'category']].to_csv('result.csv', index=False)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd2c1b2",
   "metadata": {},
   "source": [
    "### Ensemble prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3132fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices())\n",
    "# Check if TensorFlow is using the M1 Neural Engine\n",
    "print(tf.config.list_physical_devices('CPU'))\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.config.list_physical_devices('MLC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9769cb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
